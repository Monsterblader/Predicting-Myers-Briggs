{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## In this file, I create holdback set with single posts.\n",
    "## Resources\n",
    "[NLTK][https://stackabuse.com/text-classification-with-python-and-scikit-learn/]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "vectorizer_max_features = 1500\n",
    "chosen_classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_data_set\n",
    "myers_briggs = load_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_df = pd.DataFrame(myers_briggs, columns=['type', 'posts'])\n",
    "types = sorted(mb_df['type'].unique())\n",
    "\n",
    "post_list = [re.split('\\|\\|\\|+', post) for post in mb_df['posts']]\n",
    "post_df = pd.DataFrame(post_list)\n",
    "post_df.insert(loc=0, column='type', value=mb_df['type'])\n",
    "\n",
    "posts_by_type = {typ: mb_df[mb_df['type'] == typ] for typ in types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_post_df = pd.read_csv('vertical_posts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mb_df['posts'], mb_df['type']\n",
    "# X, y = vertical_post_df['posts'], vertical_post_df['type']"
   ]
  },
  {
   "source": [
    "## Might want to remove URLs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_holdback, y_train_val, y_holdback = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X_train_val)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X_train_val.iloc[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=vectorizer_max_features, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X_train_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_train_val, train_size=train_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier = chosen_classifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  0   2   0   0   0   0   0   0   9  12   2   1   0   0   0   1]\n [  0  39   0   1   0   0   0   0  17  36   8   5   0   0   0   2]\n [  0   0   7   2   0   0   0   0   6   5   3   3   0   0   0   0]\n [  0   4   0  44   0   0   0   0  12  17   6  14   0   0   1   0]\n [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0]\n [  0   0   1   0   0   0   0   0   3   3   1   1   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0]\n [  0   1   1   2   0   0   0   0   1   2   3   3   0   0   0   1]\n [  0   3   0   4   0   0   0   0 148  50   7   4   0   0   0   0]\n [  0   1   0   0   0   0   0   0  24 264   3   7   0   1   0   0]\n [  0   2   0   3   0   0   0   0  21  22 100  22   0   0   1   0]\n [  0   1   0   3   0   0   0   0   7  27  10 151   0   0   0   0]\n [  0   1   0   1   0   0   0   0   4   8   1   2   4   1   0   0]\n [  0   0   0   1   0   0   0   0   8  20   2   1   0   6   0   0]\n [  0   2   0   0   0   0   0   0   4   9   5   6   0   0   3   0]\n [  0   0   0   0   0   0   0   0   5  10   3   8   0   0   0  16]]\n              precision    recall  f1-score   support\n\n        ENFJ       0.00      0.00      0.00        27\n        ENFP       0.70      0.36      0.48       108\n        ENTJ       0.78      0.27      0.40        26\n        ENTP       0.72      0.45      0.55        98\n        ESFJ       0.00      0.00      0.00         1\n        ESFP       0.00      0.00      0.00         9\n        ESTJ       0.00      0.00      0.00         2\n        ESTP       0.00      0.00      0.00        14\n        INFJ       0.55      0.69      0.61       216\n        INFP       0.54      0.88      0.67       300\n        INTJ       0.65      0.58      0.62       171\n        INTP       0.66      0.76      0.71       199\n        ISFJ       1.00      0.18      0.31        22\n        ISFP       0.75      0.16      0.26        38\n        ISTJ       0.60      0.10      0.18        29\n        ISTP       0.80      0.38      0.52        42\n\n    accuracy                           0.60      1302\n   macro avg       0.48      0.30      0.33      1302\nweighted avg       0.61      0.60      0.57      1302\n\n0.6006144393241167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  0   2   0   0   0   0   0   0   9  12   2   1   0   0   0   1]\n [  0  39   0   1   0   0   0   0  17  36   8   5   0   0   0   2]\n [  0   0   7   2   0   0   0   0   6   5   3   3   0   0   0   0]\n [  0   4   0  44   0   0   0   0  12  17   6  14   0   0   1   0]\n [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0]\n [  0   0   1   0   0   0   0   0   3   3   1   1   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0]\n [  0   1   1   2   0   0   0   0   1   2   3   3   0   0   0   1]\n [  0   3   0   4   0   0   0   0 148  50   7   4   0   0   0   0]\n [  0   1   0   0   0   0   0   0  24 264   3   7   0   1   0   0]\n [  0   2   0   3   0   0   0   0  21  22 100  22   0   0   1   0]\n [  0   1   0   3   0   0   0   0   7  27  10 151   0   0   0   0]\n [  0   1   0   1   0   0   0   0   4   8   1   2   4   1   0   0]\n [  0   0   0   1   0   0   0   0   8  20   2   1   0   6   0   0]\n [  0   2   0   0   0   0   0   0   4   9   5   6   0   0   3   0]\n [  0   0   0   0   0   0   0   0   5  10   3   8   0   0   0  16]]\n              precision    recall  f1-score   support\n\n        ENFJ       0.00      0.00      0.00        27\n        ENFP       0.70      0.36      0.48       108\n        ENTJ       0.78      0.27      0.40        26\n        ENTP       0.72      0.45      0.55        98\n        ESFJ       0.00      0.00      0.00         1\n        ESFP       0.00      0.00      0.00         9\n        ESTJ       0.00      0.00      0.00         2\n        ESTP       0.00      0.00      0.00        14\n        INFJ       0.55      0.69      0.61       216\n        INFP       0.54      0.88      0.67       300\n        INTJ       0.65      0.58      0.62       171\n        INTP       0.66      0.76      0.71       199\n        ISFJ       1.00      0.18      0.31        22\n        ISFP       0.75      0.16      0.26        38\n        ISTJ       0.60      0.10      0.18        29\n        ISTP       0.80      0.38      0.52        42\n\n    accuracy                           0.60      1302\n   macro avg       0.48      0.30      0.33      1302\nweighted avg       0.61      0.60      0.57      1302\n\n0.6006144393241167\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 1500 0.8\n",
      "Accuracy: 0.6006144393241167\n",
      "Precision: 0.6006144393241167\n",
      "Precision: [0.         0.69642857 0.77777778 0.72131148 0.         0.\n",
      " 0.         0.         0.54814815 0.54320988 0.64935065 0.65938865\n",
      " 1.         0.75       0.6        0.8       ]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support',\n",
       " '',\n",
       " '        ENFJ       0.00      0.00      0.00        27',\n",
       " '        ENFP       0.70      0.36      0.48       108',\n",
       " '        ENTJ       0.78      0.27      0.40        26',\n",
       " '        ENTP       0.72      0.45      0.55        98',\n",
       " '        ESFJ       0.00      0.00      0.00         1',\n",
       " '        ESFP       0.00      0.00      0.00         9',\n",
       " '        ESTJ       0.00      0.00      0.00         2',\n",
       " '        ESTP       0.00      0.00      0.00        14',\n",
       " '        INFJ       0.55      0.69      0.61       216',\n",
       " '        INFP       0.54      0.88      0.67       300',\n",
       " '        INTJ       0.65      0.58      0.62       171',\n",
       " '        INTP       0.66      0.76      0.71       199',\n",
       " '        ISFJ       1.00      0.18      0.31        22',\n",
       " '        ISFP       0.75      0.16      0.26        38',\n",
       " '        ISTJ       0.60      0.10      0.18        29',\n",
       " '        ISTP       0.80      0.38      0.52        42',\n",
       " '',\n",
       " '    accuracy                           0.60      1302',\n",
       " '   macro avg       0.48      0.30      0.33      1302',\n",
       " 'weighted avg       0.61      0.60      0.57      1302',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average='micro'))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average=None))\n",
    "cr = classification_report(y_test, y_pred2)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "source": [
    "## Run model on verticalized holdbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_df = pd.DataFrame(zip(y_holdback, X_holdback), columns=('type', 'posts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_post_list = [re.split('\\|\\|\\|+', post) for post in holdback_df['posts']]\n",
    "holdback_post_df = pd.DataFrame(holdback_post_list)\n",
    "holdback_post_df.insert(loc=0, column='type', value=holdback_df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits posts of holdback set into single posts.\n",
    "def compress_posts(df):\n",
    "    result = []\n",
    "    df_length = range(len(df))\n",
    "\n",
    "    for i in df_length:\n",
    "        for j in range(58):\n",
    "            if df.iloc[i][j] != None:\n",
    "                result.append([df['type'][i], df.iloc[i][j]])\n",
    "    \n",
    "    return pd.DataFrame(result, columns=('type', 'post'))\n",
    "\n",
    "vertical_holdback = compress_posts(holdback_post_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanitize and vectorize\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(vertical_holdback)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(vertical_holdback['post'][sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_X_holdback = vectorizer.transform(documents).toarray()\n",
    "vertical_X_holdback = tfidfconverter.fit_transform(vertical_X_holdback).toarray()\n",
    "pred_holdback = classifier.predict(vertical_X_holdback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 1500 0.8\n",
      "Accuracy: 0.21017306359588747\n",
      "Precision: 0.21017306359588747\n",
      "Precision: [0.30810811 0.10579173 0.23310345 0.42866324 0.         0.\n",
      " 0.         0.1875     0.58827786 0.22969895 0.28028763 0.41295337\n",
      " 0.24545455 0.34740883 0.27010622 0.07834788]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support',\n",
       " '',\n",
       " '        ENFJ       0.31      0.07      0.11      2506',\n",
       " '        ENFP       0.11      0.37      0.16      8836',\n",
       " '        ENTJ       0.23      0.06      0.09      2915',\n",
       " '        ENTP       0.43      0.09      0.14      7753',\n",
       " '        ESFJ       0.00      0.00      0.00       525',\n",
       " '        ESFP       0.00      0.00      0.00       867',\n",
       " '        ESTJ       0.00      0.00      0.00       246',\n",
       " '        ESTP       0.19      0.03      0.05       964',\n",
       " '        INFJ       0.59      0.09      0.15     18727',\n",
       " '        INFP       0.23      0.58      0.33     20844',\n",
       " '        INTJ       0.28      0.13      0.18     13730',\n",
       " '        INTP       0.41      0.10      0.16     15965',\n",
       " '        ISFJ       0.25      0.07      0.11      1526',\n",
       " '        ISFP       0.35      0.05      0.09      3495',\n",
       " '        ISTJ       0.27      0.07      0.11      2667',\n",
       " '        ISTP       0.08      0.10      0.09      3771',\n",
       " '',\n",
       " '    accuracy                           0.21    105337',\n",
       " '   macro avg       0.23      0.11      0.11    105337',\n",
       " 'weighted avg       0.33      0.21      0.18    105337',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(vertical_holdback['type'], pred_holdback))\n",
    "print(\"Precision:\", precision_score(vertical_holdback['type'], pred_holdback, average='micro'))\n",
    "print(\"Precision:\", precision_score(vertical_holdback['type'], pred_holdback, average=None))\n",
    "cr = classification_report(vertical_holdback['type'], pred_holdback)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}