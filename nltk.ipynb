{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## In this file, I create holdback set with single posts.\n",
    "## Resources\n",
    "[NLTK][https://stackabuse.com/text-classification-with-python-and-scikit-learn/]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "vectorizer_max_features = 1500\n",
    "chosen_classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_data_set\n",
    "myers_briggs = load_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_df = pd.DataFrame(myers_briggs, columns=['type', 'posts'])\n",
    "types = sorted(mb_df['type'].unique())\n",
    "\n",
    "post_list = [re.split('\\|\\|\\|+', post) for post in mb_df['posts']]\n",
    "post_df = pd.DataFrame(post_list)\n",
    "post_df.insert(loc=0, column='type', value=mb_df['type'])\n",
    "\n",
    "posts_by_type = {typ: mb_df[mb_df['type'] == typ] for typ in types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_post_df = pd.read_csv('vertical_posts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mb_df['posts'], mb_df['type']\n",
    "# X, y = vertical_post_df['posts'], vertical_post_df['type']"
   ]
  },
  {
   "source": [
    "## Might want to remove URLs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_holdback, y_train_val, y_holdback = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X_train_val)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X_train_val.iloc[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=vectorizer_max_features, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X_train_val).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_train_val, train_size=train_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier = chosen_classifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  0   0   0   1   0   0   0   0   9   8   0   2   0   0   0   0]\n [  0  48   0   4   0   0   0   0  11  24   6   5   0   0   0   1]\n [  1   1   3   1   0   0   0   0   4   6  10   8   0   0   0   0]\n [  0   3   0  38   0   0   0   0  15   8   5  19   0   0   0   0]\n [  0   0   0   1   0   0   0   0   0   1   0   2   0   0   0   0]\n [  0   0   0   0   0   0   0   0   3   2   0   3   0   0   0   0]\n [  0   1   0   1   0   0   0   0   1   2   2   0   0   0   0   0]\n [  0   1   0   2   0   0   0   0   4   7   3   2   0   0   0   0]\n [  0   5   0   3   0   0   0   0 165  29  10   8   0   0   0   0]\n [  0   0   0   3   0   0   0   0  22 257   2  12   0   0   0   1]\n [  0   4   0   1   0   0   0   0  16  24  90  20   0   0   0   0]\n [  0   1   0   6   0   0   0   0  19  26   8 153   0   0   0   0]\n [  0   0   0   1   0   0   0   0   3  11   1   3   7   0   1   0]\n [  0   3   0   0   0   0   0   0   3  16   1   2   0   2   0   0]\n [  0   0   0   0   0   0   0   0   3  14   6   6   0   0   1   0]\n [  0   1   0   2   0   0   0   0  10  14   2  12   0   0   0  13]]\n              precision    recall  f1-score   support\n\n        ENFJ       0.00      0.00      0.00        20\n        ENFP       0.71      0.48      0.57        99\n        ENTJ       1.00      0.09      0.16        34\n        ENTP       0.59      0.43      0.50        88\n        ESFJ       0.00      0.00      0.00         4\n        ESFP       0.00      0.00      0.00         8\n        ESTJ       0.00      0.00      0.00         7\n        ESTP       0.00      0.00      0.00        19\n        INFJ       0.57      0.75      0.65       220\n        INFP       0.57      0.87      0.69       297\n        INTJ       0.62      0.58      0.60       155\n        INTP       0.60      0.72      0.65       213\n        ISFJ       1.00      0.26      0.41        27\n        ISFP       1.00      0.07      0.14        27\n        ISTJ       0.50      0.03      0.06        30\n        ISTP       0.87      0.24      0.38        54\n\n    accuracy                           0.60      1302\n   macro avg       0.50      0.28      0.30      1302\nweighted avg       0.61      0.60      0.55      1302\n\n0.5967741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  0   0   0   1   0   0   0   0   9   8   0   2   0   0   0   0]\n",
      " [  0  48   0   4   0   0   0   0  11  24   6   5   0   0   0   1]\n",
      " [  1   1   3   1   0   0   0   0   4   6  10   8   0   0   0   0]\n",
      " [  0   3   0  38   0   0   0   0  15   8   5  19   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   1   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   2   0   3   0   0   0   0]\n",
      " [  0   1   0   1   0   0   0   0   1   2   2   0   0   0   0   0]\n",
      " [  0   1   0   2   0   0   0   0   4   7   3   2   0   0   0   0]\n",
      " [  0   5   0   3   0   0   0   0 165  29  10   8   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0  22 257   2  12   0   0   0   1]\n",
      " [  0   4   0   1   0   0   0   0  16  24  90  20   0   0   0   0]\n",
      " [  0   1   0   6   0   0   0   0  19  26   8 153   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0   3  11   1   3   7   0   1   0]\n",
      " [  0   3   0   0   0   0   0   0   3  16   1   2   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   3  14   6   6   0   0   1   0]\n",
      " [  0   1   0   2   0   0   0   0  10  14   2  12   0   0   0  13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.00      0.00      0.00        20\n",
      "        ENFP       0.71      0.48      0.57        99\n",
      "        ENTJ       1.00      0.09      0.16        34\n",
      "        ENTP       0.59      0.43      0.50        88\n",
      "        ESFJ       0.00      0.00      0.00         4\n",
      "        ESFP       0.00      0.00      0.00         8\n",
      "        ESTJ       0.00      0.00      0.00         7\n",
      "        ESTP       0.00      0.00      0.00        19\n",
      "        INFJ       0.57      0.75      0.65       220\n",
      "        INFP       0.57      0.87      0.69       297\n",
      "        INTJ       0.62      0.58      0.60       155\n",
      "        INTP       0.60      0.72      0.65       213\n",
      "        ISFJ       1.00      0.26      0.41        27\n",
      "        ISFP       1.00      0.07      0.14        27\n",
      "        ISTJ       0.50      0.03      0.06        30\n",
      "        ISTP       0.87      0.24      0.38        54\n",
      "\n",
      "    accuracy                           0.60      1302\n",
      "   macro avg       0.50      0.28      0.30      1302\n",
      "weighted avg       0.61      0.60      0.55      1302\n",
      "\n",
      "0.5967741935483871\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'> 1500 0.8\nAccuracy: 0.5967741935483871\nPrecision: 0.5967741935483871\nPrecision: [0.         0.70588235 1.         0.59375    0.         0.\n 0.         0.         0.57291667 0.57238307 0.61643836 0.59533074\n 1.         1.         0.5        0.86666667]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['              precision    recall  f1-score   support',\n",
       " '',\n",
       " '        ENFJ       0.00      0.00      0.00        20',\n",
       " '        ENFP       0.71      0.48      0.57        99',\n",
       " '        ENTJ       1.00      0.09      0.16        34',\n",
       " '        ENTP       0.59      0.43      0.50        88',\n",
       " '        ESFJ       0.00      0.00      0.00         4',\n",
       " '        ESFP       0.00      0.00      0.00         8',\n",
       " '        ESTJ       0.00      0.00      0.00         7',\n",
       " '        ESTP       0.00      0.00      0.00        19',\n",
       " '        INFJ       0.57      0.75      0.65       220',\n",
       " '        INFP       0.57      0.87      0.69       297',\n",
       " '        INTJ       0.62      0.58      0.60       155',\n",
       " '        INTP       0.60      0.72      0.65       213',\n",
       " '        ISFJ       1.00      0.26      0.41        27',\n",
       " '        ISFP       1.00      0.07      0.14        27',\n",
       " '        ISTJ       0.50      0.03      0.06        30',\n",
       " '        ISTP       0.87      0.24      0.38        54',\n",
       " '',\n",
       " '    accuracy                           0.60      1302',\n",
       " '   macro avg       0.50      0.28      0.30      1302',\n",
       " 'weighted avg       0.61      0.60      0.55      1302',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average='micro'))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average=None))\n",
    "cr = classification_report(y_test, y_pred2)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_df = pd.DataFrame(zip(y_holdback, X_holdback), columns=('type', 'posts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_post_list = [re.split('\\|\\|\\|+', post) for post in holdback_df['posts']]\n",
    "holdback_post_df = pd.DataFrame(holdback_post_list)\n",
    "holdback_post_df.insert(loc=0, column='type', value=holdback_df['type'])\n",
    "\n",
    "# holdback_posts_by_type = {typ: holdback_df[holdback_df['type'] == typ] for typ in types}\n",
    "# holdback_posts_by_type_df = pd.DataFrame([holdback_posts_by_type.keys(), holdback_posts_by_type.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert post_df to a two-column data set.\n",
    "def compress_posts(df):\n",
    "    result = []\n",
    "    df_length = range(len(df))\n",
    "\n",
    "    for i in df_length:\n",
    "        for j in range(59):\n",
    "            if df.iloc[i][j] != None:\n",
    "                result.append([df['type'][i], df.iloc[i][j]])\n",
    "    \n",
    "    return pd.DataFrame(result, columns=('type', 'post'))\n",
    "\n",
    "vertical_holdback = compress_posts(holdback_post_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}