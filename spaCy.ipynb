{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Resources\n",
    "[spaCy][https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_max_features = 1500\n",
    "chosen_classifier = RandomForestClassifier\n",
    "train_size = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_data_set\n",
    "myers_briggs = load_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)\n",
    "\n",
    "parser = English()\n",
    "\n",
    "#Custom transformer using spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic utility function to clean the text \n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()"
   ]
  },
  {
   "source": [
    "## For loading posts by personality type from database."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_posts_by_type = \"SELECT posts FROM raw_data WHERE type = 'ENTJ';\"\n",
    "\n",
    "# posts_by_type = pg_fetch_all(connection, get_posts_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_df = pd.DataFrame(myers_briggs, columns=['type', 'posts'])\n",
    "types = sorted(mb_df['type'].unique())\n",
    "\n",
    "post_list = [re.split('\\|\\|\\|+', post) for post in mb_df['posts']]\n",
    "post_df = pd.DataFrame(post_list)\n",
    "post_df.insert(loc=0, column='type', value=mb_df['type'])\n",
    "\n",
    "posts_by_type = {typ: mb_df[mb_df['type'] == typ] for typ in types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# # Convert post_df to a two-column data set.\n",
    "# def compress_posts(df):\n",
    "#     alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "#     punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "#     result = []\n",
    "#     df_length = range(len(df))\n",
    "\n",
    "#     for i in df_length:\n",
    "#         for j in range(1, 59):\n",
    "#             if df.iloc[i][j] != None:\n",
    "#                 cell = punc_lower(alphanumeric(df.iloc[i][j]))\n",
    "#                 if cell != None:\n",
    "#                     result.append([df['type'][i], cell])\n",
    "    \n",
    "#     return pd.DataFrame(result)\n",
    "\n",
    "# vertical_post_df = compress_posts(post_df)\n",
    "# vertical_post_df.columns = ['type', 'posts']\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# os.system('say \"your program took %s seconds\"' % int(elapsed_time))\n",
    "# print(\"--- %s seconds ---\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_post_df = pd.read_csv('vertical_posts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "personality_count = Counter()\n",
    "\n",
    "for i in mb_df['type']:\n",
    "    personality_count[i] += 1\n",
    "\n",
    "personality_types = sorted(personality_count)\n",
    "post_count = [personality_count[x] for x in personality_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(personality_types, post_count)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spacy tokenizer that parses a sentence and generates tokens\n",
    "#these can also be replaced by word vectors \n",
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "#create vectorizer object to generate feature vectors, we will use custom spacyâ€™s tokenizer\n",
    "vectorizer = CountVectorizer(max_features=vectorizer_max_features, tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
    "classifier = chosen_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat data into [(text, type)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(mb_df['posts'], mb_df['type'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(vertical_post_df['posts'], vertical_post_df['type'], train_size=train_size)\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for i in range(len(X_train)):\n",
    "    train.append((X_train.iloc[i], y_train.iloc[i]))\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    test.append((X_test.iloc[i], y_test.iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Create model and measure accuracy\n",
    "pipe.fit([x[0] for x in train], [x[1] for x in train]) \n",
    "pred_data = pipe.predict([x[0] for x in test]) \n",
    "# for (sample, pred) in zip(test, pred_data):\n",
    "#     print(sample, pred)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "os.system('say \"your program took %s seconds\"' % int(elapsed_time))\n",
    "print(\"--- %s seconds ---\" % elapsed_time)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score([x[1] for x in test], pred_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size, elapsed_time)\n",
    "\n",
    "print(\"Precision:\", precision_score([x[1] for x in test], pred_data, average='micro'))\n",
    "print(\"Precision:\", precision_score([x[1] for x in test], pred_data, average=None))\n",
    "cr = classification_report([x[1] for x in test], pred_data)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}