{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('metis': conda)",
   "display_name": "Python 3.8.5 64-bit ('metis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1a0499c7a4a50afe9d0222578684e6b7d0a5b28e1d6168b6dad088fd14a794c9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## In this file, I create holdback set with single posts.\n",
    "## Resources\n",
    "[NLTK][https://stackabuse.com/text-classification-with-python-and-scikit-learn/]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, plot_confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (9, 6)\n",
    "sns.set(context='notebook', style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "vectorizer_max_features = 1500\n",
    "chosen_classifier = MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_data_set, sanitize_posts\n",
    "myers_briggs = load_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_df = pd.DataFrame(myers_briggs, columns=['type', 'posts'])\n",
    "types = sorted(mb_df['type'].unique())\n",
    "\n",
    "post_list = [re.split('\\|\\|\\|+', post) for post in mb_df['posts']]\n",
    "post_df = pd.DataFrame(post_list)\n",
    "post_df.insert(loc=0, column='type', value=mb_df['type'])\n",
    "\n",
    "posts_by_type = {typ: mb_df[mb_df['type'] == typ] for typ in types}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_post_df = pd.read_csv('vertical_posts.csv', index_col=0)"
   ]
  },
  {
   "source": [
    "# For Over/Under samplling\n",
    "X_nat, y_nat = mb_df['posts'], mb_df['type']\n",
    "os_qty = 400\n",
    "ros = RandomOverSampler({\"ENFJ\": os_qty, \"ENTJ\": os_qty, \"ESFJ\": os_qty, \"ESFP\": os_qty, \"ESTJ\": os_qty, \"ESTP\": os_qty, 'ISFJ': os_qty, 'ISFP': os_qty, 'ISTJ': os_qty, 'ISTP': os_qty})\n",
    "X, y = ros.fit_sample(pd.DataFrame(X_nat), y_nat)\n",
    "X_train_val, X_holdback, y_train_val, y_holdback = train_test_split(X, y)\n",
    "documents = sanitize_posts(X_train_val['posts'])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# For the original set\n",
    "X, y = mb_df['posts'], mb_df['type']\n",
    "# X, y = vertical_post_df['posts'], vertical_post_df['type']\n",
    "X_train_val, X_holdback, y_train_val, y_holdback = train_test_split(X, y)\n",
    "documents = sanitize_posts(X_train_val)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Might want to remove URLs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=vectorizer_max_features, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(X_train_val['posts']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_train_val, train_size=train_size, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier = chosen_classifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average='micro'))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred2, average=None))\n",
    "cr = classification_report(y_test, y_pred2)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "source": [
    "## Run model on verticalized holdbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_df = pd.DataFrame(zip(y_holdback, X_holdback['posts']), columns=('type', 'posts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdback_post_list = [re.split('\\|\\|\\|+', post) for post in holdback_df['posts']]\n",
    "holdback_post_df = pd.DataFrame(holdback_post_list)\n",
    "holdback_post_df.insert(loc=0, column='type', value=holdback_df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits posts of holdback set into single posts.\n",
    "def compress_posts(df):\n",
    "    result = []\n",
    "    df_length = range(len(df))\n",
    "\n",
    "    for i in df_length:\n",
    "        for j in range(58):\n",
    "            if df.iloc[i][j] != None:\n",
    "                result.append([df['type'][i], df.iloc[i][j]])\n",
    "    \n",
    "    return pd.DataFrame(result, columns=('type', 'post'))\n",
    "\n",
    "vertical_holdback = compress_posts(holdback_post_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanitize and vectorize\n",
    "documents = sanitize_posts(vertical_holdback['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_X_holdback = vectorizer.transform(documents).toarray()\n",
    "vertical_X_holdback = tfidfconverter.fit_transform(vertical_X_holdback).toarray()\n",
    "pred_holdback = classifier.predict(vertical_X_holdback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_classifier, vectorizer_max_features, train_size)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(vertical_holdback['type'], pred_holdback))\n",
    "print(\"Precision:\", precision_score(vertical_holdback['type'], pred_holdback, average='micro'))\n",
    "print(\"Precision:\", precision_score(vertical_holdback['type'], pred_holdback, average=None))\n",
    "cr = classification_report(vertical_holdback['type'], pred_holdback)\n",
    "cr.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(classifier, vertical_X_holdback, vertical_holdback['type'])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict_proba(vertical_X_holdback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "personality_count = Counter()\n",
    "\n",
    "for i in mb_df['type']:\n",
    "    personality_count[i] += 1\n",
    "\n",
    "personality_types = sorted(personality_count)\n",
    "post_count = [personality_count[x] for x in personality_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(personality_types, post_count)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Personality Type')\n",
    "plt.ylabel('Number of Posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert = \"Hi, everyone!  Iâ€™m a San Francisco native who attended Caltech in Pasadena and has spent time all over the country.  My favorite cities are San Francisco, Boston, Raleigh, and Denver.  I am a bootcamp veteran, having acquired a skill set in web development, and where I, amazingly, met Josh Shaman who now works for Metis.  I bike, play piano, and dance in my spare time.\"\n",
    "\n",
    "trans_albert = vectorizer.transform([albert]).toarray()\n",
    "trans_albert = tfidfconverter.fit_transform(trans_albert).toarray()\n",
    "classifier.predict(trans_albert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}